## scikit-learn 学习笔记

### 一. 简明使用指南

**1. 加载样例数据**

  以自带的标准数据集iris数据集（类似还有波士顿房价， 手写数字等）为例

  > 官方代码示例


  ``` Python
  from sklearn import datasets
  iris = datasets.load_iris()
  digits = datasets.load_digits()

  print(digits.data)
  [output]：
  [[ 0.  0.  5. ...  0.  0.  0.]
   [ 0.  0.  0. ... 10.  0.  0.]
   [ 0.  0.  0. ... 16.  9.  0.]
   ...
   [ 0.  0.  1. ...  6.  0.  0.]
   [ 0.  0.  2. ... 12.  0.  0.]
   [ 0.  0. 10. ... 12.  1.  0.]]

  ```

**2.数据读取接口**

- 通用API：

- 样本生成：

    分为单标签和多标签，使用内置（随机）生成函数生成，还可生成用于聚类、回归、流形学习、重构的数据集。

- 用于SVM的数据集

- **加载来自外部的数据集：**

    scikit-learn可以对存储为numpy数组或scipy稀疏矩阵的任何数字数据进行学习。其他可转换为数字数组的类型的数据，例如pandas的DataFrame数据，也可以接受。

    利用pandas.io scipy.io numpy.io等进行加载


**3.针对数据集的操作**

  scikit-learn库提供了数据转换工具，可以实现数据清洗、降维、扩展、生成以及特征提取等操作。


**4.有监督学习方法**

**随机森林：**

随机森林是一种元估计器，它适用于数据集的各种子样本上的许多决策树分类器，并使用**平均**来提高预测精度和控制过度拟合。子样本大小始终与原始输入示例的大小相同，但如果bootstrap=True(默认)，则对样本进行抽取操作。

ET或Extra-Trees（Extremely randomized trees，极端随机树）是由PierreGeurts等人于2006年提出。该算法与随机森林算法十分相似，都是由许多决策树构成。但该算法与随机森林有两点主要的区别：

- 随机森林应用的是Bagging模型，而ET是使用所有的训练样本得到每棵决策树，也就是每棵决策树应用的是相同的全部训练样本；

- 随机森林是在一个随机子集内得到最佳分叉属性，而ET是完全随机的得到分叉值，从而实现对决策树进行分叉的。

  _**Extremely Randomized Trees**_

  在极端随机树中，随机性更进一步。在随机森林中，使用随机的候选特征子集，但不是寻找最具鉴别性的阈值，而是对每个候选特征随机抽取阈值，并将这些随机生成的阈值中的最佳值作为分割规则进行选择。这通常可以使模型的方差(越大表示模型的预测能力越不稳定)减少一点，但会使偏差（越小表示模型预测能力越好，类比有偏无偏）增加一点:

  > 官方代码示例

  ``` Python

    from sklearn.model_selection import cross_val_score
    from sklearn.datasets import make_blobs
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.ensemble import ExtraTreesClassifier
    from sklearn.tree import DecisionTreeClassifier

    X, y = make_blobs(n_samples=10000, n_features=10, centers=100, random_state=0)
    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)
    scores = cross_val_score(clf, X, y)
    print(scores.mean())  #决策树的模型精准度得分

    clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)
    scores = cross_val_score(clf, X, y)
    print(scores.mean())  #决策树的模型精准度得分

    clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)
    scores = cross_val_score(clf, X, y)
    print(scores.mean())  #随机森林（普通随机树）的模型精准度得分

    clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)
    scores = cross_val_score(clf, X, y)
    print(scores.mean())  #随机森林（极端随机数）的模型精准度得分

    '''
    可以看到ET的得分最高
    '''
    [output]：
    0.9794087938205586
    0.9996078431372549
    0.99989898989899

  ```

- 参数说明：

  n_estimators：森林中子树的数目，数目越多性能越好但是计算强度也越大

  max_features: 在拆分节点时考虑的随机特征子集的大小，子集越小，模型方差减小越多，但是偏差也会相应增大。

  **按照经验来说，对于回归任务 默认值是`max_features=n_features`; 对于分类任务， `max_features=sqrt(n_features)`** n_features是数据特征的数量（维度）

  max_depth参数常常设置为None, 同时min_samples_split参数设置为1（充分利用森林中的树）

  以上只是经验之谈，好的模型参数需要进行大量的交叉检验。一般的

    - **随机森林中bootstrap参数设置为True**

    - ET中需要用到全部的数据集，所以`bootstrap=False`

    - 当使用bootstrap采样时，可以估计在遗漏的样本或袋外样本上的泛化精度。这可以通过设置oob_score=True（**袋外准确率**）来启用。


  - 模型的容量：M\*N\*log(N)

    其中M是子树的个数，N是样本的数量，模型大小可以通过以下参数调节 `min_samples_split, min_samples_leaf, max_leaf_nodes and max_depth`

    
